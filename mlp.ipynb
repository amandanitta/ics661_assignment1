{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>5</th>\n",
       "      <th>0</th>\n",
       "      <th>0.1</th>\n",
       "      <th>0.2</th>\n",
       "      <th>0.3</th>\n",
       "      <th>0.4</th>\n",
       "      <th>0.5</th>\n",
       "      <th>0.6</th>\n",
       "      <th>0.7</th>\n",
       "      <th>0.8</th>\n",
       "      <th>...</th>\n",
       "      <th>0.608</th>\n",
       "      <th>0.609</th>\n",
       "      <th>0.610</th>\n",
       "      <th>0.611</th>\n",
       "      <th>0.612</th>\n",
       "      <th>0.613</th>\n",
       "      <th>0.614</th>\n",
       "      <th>0.615</th>\n",
       "      <th>0.616</th>\n",
       "      <th>0.617</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>26847</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25478</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16196</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31483</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13960</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54343</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38158</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>860</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15795</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56422</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>47999 rows Ã— 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       5  0  0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  ...  0.608  0.609  0.610  \\\n",
       "26847  6  0    0    0    0    0    0    0    0    0  ...      0      0      0   \n",
       "25478  4  0    0    0    0    0    0    0    0    0  ...      0      0      0   \n",
       "16196  7  0    0    0    0    0    0    0    0    0  ...      0      0      0   \n",
       "31483  4  0    0    0    0    0    0    0    0    0  ...      0      0      0   \n",
       "13960  1  0    0    0    0    0    0    0    0    0  ...      0      0      0   \n",
       "...   .. ..  ...  ...  ...  ...  ...  ...  ...  ...  ...    ...    ...    ...   \n",
       "54343  8  0    0    0    0    0    0    0    0    0  ...      0      0      0   \n",
       "38158  1  0    0    0    0    0    0    0    0    0  ...      0      0      0   \n",
       "860    3  0    0    0    0    0    0    0    0    0  ...      0      0      0   \n",
       "15795  2  0    0    0    0    0    0    0    0    0  ...      0      0      0   \n",
       "56422  1  0    0    0    0    0    0    0    0    0  ...      0      0      0   \n",
       "\n",
       "       0.611  0.612  0.613  0.614  0.615  0.616  0.617  \n",
       "26847      0      0      0      0      0      0      0  \n",
       "25478      0      0      0      0      0      0      0  \n",
       "16196      0      0      0      0      0      0      0  \n",
       "31483      0      0      0      0      0      0      0  \n",
       "13960      0      0      0      0      0      0      0  \n",
       "...      ...    ...    ...    ...    ...    ...    ...  \n",
       "54343      0      0      0      0      0      0      0  \n",
       "38158      0      0      0      0      0      0      0  \n",
       "860        0      0      0      0      0      0      0  \n",
       "15795      0      0      0      0      0      0      0  \n",
       "56422      0      0      0      0      0      0      0  \n",
       "\n",
       "[47999 rows x 785 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_og = pd.read_csv('data/train.csv')\n",
    "\n",
    "test = pd.read_csv('data/test.csv')\n",
    "\n",
    "display(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, validation = train_test_split(train_og,train_size=0.8,random_state=42)\n",
    "\n",
    "y_train = train.iloc[:,0]\n",
    "X_train = train.drop(train.columns[0],axis=1)\n",
    "\n",
    "y_validation = validation.iloc[:,0]\n",
    "X_validation = validation.drop(validation.columns[0],axis=1)\n",
    "\n",
    "y_test = test.iloc[:,0]\n",
    "X_test = test.drop(test.columns[0],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.ops as ops\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data(X_train, y_train, X_validation, y_validation, X_test, y_test):\n",
    "    X_train_tensors = torch.tensor(X_train.values,dtype=torch.float32)\n",
    "    y_train_tensors = torch.tensor(y_train.values,dtype=torch.long)\n",
    "\n",
    "    dataset_train = TensorDataset(X_train_tensors,y_train_tensors)\n",
    "    train_loader = DataLoader(dataset_train,batch_size=64,shuffle=True)\n",
    "\n",
    "    X_validation_tensors = torch.tensor(X_validation.values,dtype=torch.float32)\n",
    "    y_validation_tensors = torch.tensor(y_validation.values,dtype=torch.long)\n",
    "\n",
    "    X_test_tensors = torch.tensor(X_test.values,dtype=torch.float32)\n",
    "    y_test_tensors = torch.tensor(y_test.values,dtype=torch.long)\n",
    "\n",
    "    return train_loader, X_validation_tensors, y_validation_tensors, X_test_tensors, y_test_tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, X_validation_tensors, y_validation_tensors, X_test_tensors, y_test_tensors = create_data(X_train, y_train, X_validation, y_validation, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1800x500 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(18, 5))\n",
    "for i in range(3):\n",
    "    plt.subplot(1, 3, i + 1)\n",
    "    plt.imshow(X_train.iloc[i].values.reshape(28,28), cmap='gray')\n",
    "    plt.subplots_adjust(wspace=0.2, hspace=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_size = 10\n",
    "input_size = X_train.columns[0:].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Linear(input_size,28), # input\n",
    "    nn.ReLU(), # activation\n",
    "    nn.Linear(28,32), # hidden layer\n",
    "    nn.ReLU(), # activation\n",
    "    nn.Linear(32,output_size), # ouput\n",
    ")\n",
    "\n",
    "criterion = nn.CrossEntropyLoss() # loss function\n",
    "optimizer = optim.Adam(model.parameters(),lr=0.001) # optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_nn(model,loader):\n",
    "    num_epochs = 100\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  # Set model to training mode\n",
    "\n",
    "        # Variables to accumulate predictions and true labels\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "\n",
    "        for X_batch, y_batch in loader:\n",
    "            # Forward pass: compute the output of the model\n",
    "            outputs = model(X_batch)\n",
    "            \n",
    "            # Compute the loss\n",
    "            loss = criterion(outputs, y_batch)\n",
    "\n",
    "            # Backward pass: compute gradients\n",
    "            optimizer.zero_grad()  # Clear previous gradients\n",
    "            loss.backward()        # Backpropagation\n",
    "\n",
    "            # Update the weights\n",
    "            optimizer.step()\n",
    "\n",
    "            # Convert model outputs to predicted class indices\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "            # Accumulate predictions and true labels\n",
    "            all_preds.extend(predicted.cpu().numpy())  # Convert to numpy and store\n",
    "            all_labels.extend(y_batch.cpu().numpy())   # Convert to numpy and store\n",
    "\n",
    "        # Compute metrics at the end of the epoch\n",
    "        accuracy = accuracy_score(all_labels, all_preds)\n",
    "        f1 = f1_score(all_labels, all_preds, average='weighted')  # For multi-class problems\n",
    "        recall = recall_score(all_labels,all_preds, average='weighted')\n",
    "\n",
    "        # Print loss, accuracy, and F1 score every 10 epochs\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}, Accuracy: {accuracy:.4f}, F1 Score: {f1:.4f}')\n",
    "        \n",
    "    print(f'Training Loss: {loss.item():.4f}, Accuracy: {accuracy:.4f}, F1: {f1:.4f}, Recall: {recall}')\n",
    "    #return loss.item(), accuracy, f1, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Loss: 0.0529, Accuracy: 0.9581, F1 Score: 0.9581\n",
      "Epoch [20/100], Loss: 0.0435, Accuracy: 0.9688, F1 Score: 0.9688\n",
      "Epoch [30/100], Loss: 0.1873, Accuracy: 0.9744, F1 Score: 0.9744\n",
      "Epoch [40/100], Loss: 0.0933, Accuracy: 0.9767, F1 Score: 0.9767\n",
      "Epoch [50/100], Loss: 0.0232, Accuracy: 0.9788, F1 Score: 0.9788\n",
      "Epoch [60/100], Loss: 0.0173, Accuracy: 0.9804, F1 Score: 0.9804\n",
      "Epoch [70/100], Loss: 0.0670, Accuracy: 0.9811, F1 Score: 0.9811\n",
      "Epoch [80/100], Loss: 0.1157, Accuracy: 0.9837, F1 Score: 0.9837\n",
      "Epoch [90/100], Loss: 0.2069, Accuracy: 0.9825, F1 Score: 0.9825\n",
      "Epoch [100/100], Loss: 0.0427, Accuracy: 0.9846, F1 Score: 0.9846\n",
      "Training Loss: 0.0427, Accuracy: 0.9846, F1: 0.9846, Recall: 0.9845830121460863\n"
     ]
    }
   ],
   "source": [
    "train_nn(model,train_loader) # train experiment 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation_nn(model, X_set, y_set):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():  # No need to calculate gradients for validation/testing\n",
    "        outputs = model(X_set)\n",
    "        val_loss = criterion(outputs, y_set)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        all_preds.extend(predicted.cpu().numpy())  # Convert to numpy and store\n",
    "        all_labels.extend(y_set.cpu().numpy())   # Convert to numpy and store\n",
    "        accuracy = accuracy_score(all_labels, all_preds)\n",
    "        f1 = f1_score(all_labels, all_preds, average='weighted')  # For multi-class problems\n",
    "        recall = recall_score(all_labels,all_preds, average='weighted')\n",
    "        \n",
    "        print(f'Loss: {val_loss.item():.4f}, Accuracy: {accuracy:.4f}, F1 Score: {f1:.4f}, Recall: {recall:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.4686, Accuracy: 0.9443, F1 Score: 0.9442, Recall: 0.9443\n"
     ]
    }
   ],
   "source": [
    "evaluation_nn(model,X_validation_tensors, y_validation_tensors) # evaluation on validation set experiment 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.4879, Accuracy: 0.9490, F1 Score: 0.9489, Recall: 0.9490\n"
     ]
    }
   ],
   "source": [
    "evaluation_nn(model,X_test_tensors, y_test_tensors) # evaluation of test set experiment 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2 = nn.Sequential(\n",
    "    nn.Linear(input_size,28), # input\n",
    "    nn.ReLU(), # activation\n",
    "    nn.Linear(28,32), # hidden layer\n",
    "    nn.ReLU(), # activation\n",
    "    nn.Linear(32,20),\n",
    "    nn.ReLU(), # activation\n",
    "    nn.Linear(20,output_size), # ouput\n",
    ")\n",
    "\n",
    "criterion = nn.CrossEntropyLoss() # loss function\n",
    "optimizer = optim.Adam(model_2.parameters(),lr=0.001) # optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Loss: 0.0244, Accuracy: 0.9713, F1 Score: 0.9713\n",
      "Epoch [20/100], Loss: 0.0221, Accuracy: 0.9804, F1 Score: 0.9804\n",
      "Epoch [30/100], Loss: 0.0030, Accuracy: 0.9842, F1 Score: 0.9842\n",
      "Epoch [40/100], Loss: 0.0011, Accuracy: 0.9873, F1 Score: 0.9873\n",
      "Epoch [50/100], Loss: 0.0311, Accuracy: 0.9889, F1 Score: 0.9889\n",
      "Epoch [60/100], Loss: 0.0083, Accuracy: 0.9910, F1 Score: 0.9910\n",
      "Epoch [70/100], Loss: 0.0206, Accuracy: 0.9921, F1 Score: 0.9921\n",
      "Epoch [80/100], Loss: 0.1259, Accuracy: 0.9928, F1 Score: 0.9928\n",
      "Epoch [90/100], Loss: 0.0094, Accuracy: 0.9916, F1 Score: 0.9916\n",
      "Epoch [100/100], Loss: 0.0076, Accuracy: 0.9923, F1 Score: 0.9923\n",
      "Training Loss: 0.0076, Accuracy: 0.9923, F1: 0.9923, Recall: 0.992270672305673\n"
     ]
    }
   ],
   "source": [
    "train_nn(model_2,train_loader) # train on experiment 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.3757, Accuracy: 0.9590, F1 Score: 0.9590, Recall: 0.9590\n"
     ]
    }
   ],
   "source": [
    "evaluation_nn(model_2,X_validation_tensors,y_validation_tensors) # validation experiment 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.3114, Accuracy: 0.9624, F1 Score: 0.9624, Recall: 0.9624\n"
     ]
    }
   ],
   "source": [
    "evaluation_nn(model_2,X_test_tensors,y_test_tensors) # validation experiment 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_3 = nn.Sequential(\n",
    "    nn.Linear(input_size,28), # input\n",
    "    nn.ReLU(), # activation\n",
    "    nn.Linear(28,32), # hidden layer\n",
    "    nn.Sigmoid(), # activation\n",
    "    nn.Linear(32,20),\n",
    "    nn.ReLU(), # activation\n",
    "    nn.Linear(20,output_size), # ouput\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Loss: 2.3416, Accuracy: 0.0930, F1 Score: 0.0343\n",
      "Epoch [20/100], Loss: 2.3335, Accuracy: 0.0930, F1 Score: 0.0343\n",
      "Epoch [30/100], Loss: 2.3198, Accuracy: 0.0930, F1 Score: 0.0343\n",
      "Epoch [40/100], Loss: 2.3750, Accuracy: 0.0930, F1 Score: 0.0343\n",
      "Epoch [50/100], Loss: 2.3524, Accuracy: 0.0930, F1 Score: 0.0343\n",
      "Epoch [60/100], Loss: 2.3348, Accuracy: 0.0930, F1 Score: 0.0343\n",
      "Epoch [70/100], Loss: 2.3233, Accuracy: 0.0930, F1 Score: 0.0343\n",
      "Epoch [80/100], Loss: 2.3550, Accuracy: 0.0930, F1 Score: 0.0343\n",
      "Epoch [90/100], Loss: 2.3485, Accuracy: 0.0930, F1 Score: 0.0343\n",
      "Epoch [100/100], Loss: 2.3168, Accuracy: 0.0930, F1 Score: 0.0343\n",
      "Training Loss: 2.3168, Accuracy: 0.0930, F1: 0.0343, Recall: 0.09304360507510573\n"
     ]
    }
   ],
   "source": [
    "train_nn(model_3,train_loader) # train on experiment 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_4 = nn.Sequential(\n",
    "    nn.Linear(input_size,28), # input\n",
    "    nn.ReLU(), # activation\n",
    "    nn.Linear(28,32), # hidden layer\n",
    "    nn.Tanh(), # activation\n",
    "    nn.Linear(32,20),\n",
    "    nn.ReLU(), # activation\n",
    "    nn.Linear(20,output_size), # ouput\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Loss: 2.3977, Accuracy: 0.0802, F1 Score: 0.0198\n",
      "Epoch [20/100], Loss: 2.4080, Accuracy: 0.0802, F1 Score: 0.0198\n",
      "Epoch [30/100], Loss: 2.3662, Accuracy: 0.0802, F1 Score: 0.0198\n",
      "Epoch [40/100], Loss: 2.3977, Accuracy: 0.0802, F1 Score: 0.0198\n",
      "Epoch [50/100], Loss: 2.4433, Accuracy: 0.0802, F1 Score: 0.0198\n",
      "Epoch [60/100], Loss: 2.3930, Accuracy: 0.0802, F1 Score: 0.0198\n",
      "Epoch [70/100], Loss: 2.4063, Accuracy: 0.0802, F1 Score: 0.0198\n",
      "Epoch [80/100], Loss: 2.4281, Accuracy: 0.0802, F1 Score: 0.0198\n",
      "Epoch [90/100], Loss: 2.4173, Accuracy: 0.0802, F1 Score: 0.0198\n",
      "Epoch [100/100], Loss: 2.3684, Accuracy: 0.0802, F1 Score: 0.0198\n",
      "Training Loss: 2.3684, Accuracy: 0.0802, F1: 0.0198, Recall: 0.08016833684035084\n"
     ]
    }
   ],
   "source": [
    "train_nn(model_4,train_loader) # train on experiment 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9999, 784)\n",
      "(47999, 784)\n",
      "(12000, 784)\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(X_test.shape)\n",
    "print(X_train.shape)\n",
    "print(X_validation.shape)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "print(((np.isnan(X_test).sum() > 0) == True).sum())  # Check for NaNs\n",
    "print(((np.isinf(X_test).sum() > 0) == True).sum())  # Check for Infinity values\n",
    "\n",
    "X_train = X_train.astype(np.float64)\n",
    "X_validation = X_validation.astype(np.float64)\n",
    "X_test = X_test.astype(np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /opt/homebrew/anaconda3/lib/python3.12/site-packages (1.26.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        float64\n",
      "0.1      float64\n",
      "0.2      float64\n",
      "0.3      float64\n",
      "0.4      float64\n",
      "          ...   \n",
      "0.663    float64\n",
      "0.664    float64\n",
      "0.665    float64\n",
      "0.666    float64\n",
      "0.667    float64\n",
      "Length: 784, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(X_test.dtypes)  # Check the data types of each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The feature names should match those that were passed during fit.\nFeature names unseen at fit time:\n- 0.618\n- 0.619\n- 0.620\n- 0.621\n- 0.622\n- ...\nFeature names seen at fit time, yet now missing:\n- 1.1\n- 107\n- 108\n- 11\n- 11.1\n- ...\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/amandanitta/Desktop/GitHub/ICS661/ics661_assignment1/mlp.ipynb Cell 29\u001b[0m line \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/amandanitta/Desktop/GitHub/ICS661/ics661_assignment1/mlp.ipynb#X41sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m X_train_mm \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(Min_Max\u001b[39m.\u001b[39mfit_transform(X_train))\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/amandanitta/Desktop/GitHub/ICS661/ics661_assignment1/mlp.ipynb#X41sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m X_validation_mm \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(Min_Max\u001b[39m.\u001b[39mtransform(X_validation))\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/amandanitta/Desktop/GitHub/ICS661/ics661_assignment1/mlp.ipynb#X41sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m X_test_mm \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(Min_Max\u001b[39m.\u001b[39mtransform(X_test))\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.12/site-packages/sklearn/utils/_set_output.py:295\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[39m@wraps\u001b[39m(f)\n\u001b[1;32m    294\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped\u001b[39m(\u001b[39mself\u001b[39m, X, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> 295\u001b[0m     data_to_wrap \u001b[39m=\u001b[39m f(\u001b[39mself\u001b[39m, X, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    296\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data_to_wrap, \u001b[39mtuple\u001b[39m):\n\u001b[1;32m    297\u001b[0m         \u001b[39m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    298\u001b[0m         return_tuple \u001b[39m=\u001b[39m (\n\u001b[1;32m    299\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[39m0\u001b[39m], X, \u001b[39mself\u001b[39m),\n\u001b[1;32m    300\u001b[0m             \u001b[39m*\u001b[39mdata_to_wrap[\u001b[39m1\u001b[39m:],\n\u001b[1;32m    301\u001b[0m         )\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.12/site-packages/sklearn/preprocessing/_data.py:534\u001b[0m, in \u001b[0;36mMinMaxScaler.transform\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    530\u001b[0m check_is_fitted(\u001b[39mself\u001b[39m)\n\u001b[1;32m    532\u001b[0m xp, _ \u001b[39m=\u001b[39m get_namespace(X)\n\u001b[0;32m--> 534\u001b[0m X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_data(\n\u001b[1;32m    535\u001b[0m     X,\n\u001b[1;32m    536\u001b[0m     copy\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcopy,\n\u001b[1;32m    537\u001b[0m     dtype\u001b[39m=\u001b[39m_array_api\u001b[39m.\u001b[39msupported_float_dtypes(xp),\n\u001b[1;32m    538\u001b[0m     force_all_finite\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mallow-nan\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    539\u001b[0m     reset\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    540\u001b[0m )\n\u001b[1;32m    542\u001b[0m X \u001b[39m*\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscale_\n\u001b[1;32m    543\u001b[0m X \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmin_\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.12/site-packages/sklearn/base.py:608\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[1;32m    537\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_validate_data\u001b[39m(\n\u001b[1;32m    538\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    539\u001b[0m     X\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mno_validation\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    544\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcheck_params,\n\u001b[1;32m    545\u001b[0m ):\n\u001b[1;32m    546\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Validate input data and set or check the `n_features_in_` attribute.\u001b[39;00m\n\u001b[1;32m    547\u001b[0m \n\u001b[1;32m    548\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    606\u001b[0m \u001b[39m        validated.\u001b[39;00m\n\u001b[1;32m    607\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 608\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_feature_names(X, reset\u001b[39m=\u001b[39mreset)\n\u001b[1;32m    610\u001b[0m     \u001b[39mif\u001b[39;00m y \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_tags()[\u001b[39m\"\u001b[39m\u001b[39mrequires_y\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[1;32m    611\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    612\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mThis \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m estimator \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    613\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mrequires y to be passed, but the target y is None.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    614\u001b[0m         )\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.12/site-packages/sklearn/base.py:535\u001b[0m, in \u001b[0;36mBaseEstimator._check_feature_names\u001b[0;34m(self, X, reset)\u001b[0m\n\u001b[1;32m    530\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m missing_names \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m unexpected_names:\n\u001b[1;32m    531\u001b[0m     message \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (\n\u001b[1;32m    532\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFeature names must be in the same order as they were in fit.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    533\u001b[0m     )\n\u001b[0;32m--> 535\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(message)\n",
      "\u001b[0;31mValueError\u001b[0m: The feature names should match those that were passed during fit.\nFeature names unseen at fit time:\n- 0.618\n- 0.619\n- 0.620\n- 0.621\n- 0.622\n- ...\nFeature names seen at fit time, yet now missing:\n- 1.1\n- 107\n- 108\n- 11\n- 11.1\n- ...\n"
     ]
    }
   ],
   "source": [
    "Min_Max = MinMaxScaler()\n",
    "X_train_mm = pd.DataFrame(Min_Max.fit_transform(X_train))\n",
    "X_validation_mm = pd.DataFrame(Min_Max.transform(X_validation))\n",
    "X_test_mm = pd.DataFrame(Min_Max.transform(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader_mm, X_validation_tensors_mm, y_validation_tensors_mm, X_test_tensors_mm, y_test_tensors_mm = create_data(X_train_mm, y_train, X_validation_mm, y_validation, X_test_mm, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Loss: 2.6287, Accuracy: 0.1262, F1 Score: 0.0527\n",
      "Epoch [20/100], Loss: 2.5603, Accuracy: 0.1262, F1 Score: 0.0527\n",
      "Epoch [30/100], Loss: 2.5738, Accuracy: 0.1262, F1 Score: 0.0527\n",
      "Epoch [40/100], Loss: 2.7697, Accuracy: 0.1262, F1 Score: 0.0527\n",
      "Epoch [50/100], Loss: 2.3558, Accuracy: 0.1262, F1 Score: 0.0527\n",
      "Epoch [60/100], Loss: 2.6017, Accuracy: 0.1262, F1 Score: 0.0527\n",
      "Epoch [70/100], Loss: 2.2423, Accuracy: 0.1262, F1 Score: 0.0527\n",
      "Epoch [80/100], Loss: 3.0653, Accuracy: 0.1262, F1 Score: 0.0527\n",
      "Epoch [90/100], Loss: 2.7776, Accuracy: 0.1262, F1 Score: 0.0527\n",
      "Epoch [100/100], Loss: 2.8709, Accuracy: 0.1262, F1 Score: 0.0527\n",
      "Training Loss: 2.8709, Accuracy: 0.1262, F1: 0.0527, Recall: 0.12619532044760937\n"
     ]
    }
   ],
   "source": [
    "train_nn(model,train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.6355, Accuracy: 0.1194, F1 Score: 0.0518, Recall: 0.1194\n"
     ]
    }
   ],
   "source": [
    "evaluation_nn(model, X_validation_tensors_mm,y_validation_tensors_mm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.6353, Accuracy: 0.1259, F1 Score: 0.0564, Recall: 0.1259\n"
     ]
    }
   ],
   "source": [
    "evaluation_nn(model,X_test_tensors_mm,y_test_tensors_mm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "standard = StandardScaler()\n",
    "X_train_ss = pd.DataFrame(standard.fit_transform(X_train))\n",
    "X_validation_ss = pd.DataFrame(standard.fit_transform(X_validation))\n",
    "X_test_ss = pd.DataFrame(standard.fit_transform(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader_ss, X_validation_tensors_ss, y_validation_tensors_ss, X_test_tensors_ss, y_test_tensors_ss = create_data(X_train_ss, y_train, X_validation_ss, y_validation, X_test_ss, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Loss: 1.9187, Accuracy: 0.3510, F1 Score: 0.3180\n",
      "Epoch [20/100], Loss: 1.4353, Accuracy: 0.3510, F1 Score: 0.3180\n",
      "Epoch [30/100], Loss: 2.5542, Accuracy: 0.3510, F1 Score: 0.3180\n",
      "Epoch [40/100], Loss: 1.9252, Accuracy: 0.3510, F1 Score: 0.3180\n",
      "Epoch [50/100], Loss: 1.1325, Accuracy: 0.3510, F1 Score: 0.3180\n",
      "Epoch [60/100], Loss: 1.5064, Accuracy: 0.3510, F1 Score: 0.3180\n",
      "Epoch [70/100], Loss: 1.8451, Accuracy: 0.3510, F1 Score: 0.3180\n",
      "Epoch [80/100], Loss: 1.4807, Accuracy: 0.3510, F1 Score: 0.3180\n",
      "Epoch [90/100], Loss: 1.6844, Accuracy: 0.3510, F1 Score: 0.3180\n",
      "Epoch [100/100], Loss: 1.9156, Accuracy: 0.3510, F1 Score: 0.3180\n",
      "Training Loss: 1.9156, Accuracy: 0.3510, F1: 0.3180, Recall: 0.3510172939979654\n"
     ]
    }
   ],
   "source": [
    "train_nn(model,train_loader_ss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1724024966378151"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_data = train.shape[0] + test.shape[0]\n",
    "\n",
    "test.shape[0]/total_data\n",
    "\n",
    "# % * whole = part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12000, 785)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# % * whole = part\n",
    "\n",
    "validation.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8275975033621848"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape[0]/total_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.20690368633401152"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation.shape[0]/total_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(59999, 785)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_og.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
