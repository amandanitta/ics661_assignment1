{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>5</th>\n",
       "      <th>0</th>\n",
       "      <th>0.1</th>\n",
       "      <th>0.2</th>\n",
       "      <th>0.3</th>\n",
       "      <th>0.4</th>\n",
       "      <th>0.5</th>\n",
       "      <th>0.6</th>\n",
       "      <th>0.7</th>\n",
       "      <th>0.8</th>\n",
       "      <th>...</th>\n",
       "      <th>0.608</th>\n",
       "      <th>0.609</th>\n",
       "      <th>0.610</th>\n",
       "      <th>0.611</th>\n",
       "      <th>0.612</th>\n",
       "      <th>0.613</th>\n",
       "      <th>0.614</th>\n",
       "      <th>0.615</th>\n",
       "      <th>0.616</th>\n",
       "      <th>0.617</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59994</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59995</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59996</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59997</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59998</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>59999 rows Ã— 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       5  0  0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  ...  0.608  0.609  0.610  \\\n",
       "0      0  0    0    0    0    0    0    0    0    0  ...      0      0      0   \n",
       "1      4  0    0    0    0    0    0    0    0    0  ...      0      0      0   \n",
       "2      1  0    0    0    0    0    0    0    0    0  ...      0      0      0   \n",
       "3      9  0    0    0    0    0    0    0    0    0  ...      0      0      0   \n",
       "4      2  0    0    0    0    0    0    0    0    0  ...      0      0      0   \n",
       "...   .. ..  ...  ...  ...  ...  ...  ...  ...  ...  ...    ...    ...    ...   \n",
       "59994  8  0    0    0    0    0    0    0    0    0  ...      0      0      0   \n",
       "59995  3  0    0    0    0    0    0    0    0    0  ...      0      0      0   \n",
       "59996  5  0    0    0    0    0    0    0    0    0  ...      0      0      0   \n",
       "59997  6  0    0    0    0    0    0    0    0    0  ...      0      0      0   \n",
       "59998  8  0    0    0    0    0    0    0    0    0  ...      0      0      0   \n",
       "\n",
       "       0.611  0.612  0.613  0.614  0.615  0.616  0.617  \n",
       "0          0      0      0      0      0      0      0  \n",
       "1          0      0      0      0      0      0      0  \n",
       "2          0      0      0      0      0      0      0  \n",
       "3          0      0      0      0      0      0      0  \n",
       "4          0      0      0      0      0      0      0  \n",
       "...      ...    ...    ...    ...    ...    ...    ...  \n",
       "59994      0      0      0      0      0      0      0  \n",
       "59995      0      0      0      0      0      0      0  \n",
       "59996      0      0      0      0      0      0      0  \n",
       "59997      0      0      0      0      0      0      0  \n",
       "59998      0      0      0      0      0      0      0  \n",
       "\n",
       "[59999 rows x 785 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train = pd.read_csv('data/train.csv')\n",
    "\n",
    "test = pd.read_csv('data/test.csv')\n",
    "\n",
    "display(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, validation = train_test_split(train,train_size=0.8,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train.iloc[:,0]\n",
    "X_train = train.drop(train.columns[0],axis=1)\n",
    "\n",
    "y_validation = validation.iloc[:,0]\n",
    "X_validation = validation.drop(validation.columns[0],axis=1)\n",
    "\n",
    "y_test = test.iloc[:,0]\n",
    "X_test = test.drop(test.columns[0],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.ops as ops\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tensors = torch.tensor(X_train.values,dtype=torch.float32)\n",
    "y_train_tensors = torch.tensor(y_train.values,dtype=torch.long)\n",
    "\n",
    "dataset_train = TensorDataset(X_train_tensors,y_train_tensors)\n",
    "train_loader = DataLoader(dataset_train,batch_size=64,shuffle=True)\n",
    "\n",
    "X_validation_tensors = torch.tensor(X_validation.values,dtype=torch.float32)\n",
    "y_validation_tensors = torch.tensor(y_validation.values,dtype=torch.long)\n",
    "\n",
    "dataset_validation = TensorDataset(X_validation_tensors,y_validation_tensors)\n",
    "validation_loader = DataLoader(dataset_validation)\n",
    "\n",
    "X_test_tensors = torch.tensor(X_test.values,dtype=torch.float32)\n",
    "y_test_tensors = torch.tensor(y_test.values,dtype=torch.long)\n",
    "\n",
    "dataset_test = TensorDataset(X_test_tensors,y_test_tensors)\n",
    "test_loader = DataLoader(dataset_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABYkAAAGsCAYAAACYZSi6AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8fJSN1AAAACXBIWXMAAA9hAAAPYQGoP6dpAAArEklEQVR4nO3dbZCV9Xk/8OsouipddkqQfQjLzk5H04461oDlQaNo60Y6sTEEMTHpYF/YpEGmiKmJcVpJX0jGRDRTEm1tanXqs2OMHU2UjIIawAKjhhrH4oBhq+wQqe4i6lLk/r/o342Hw8Ke2/O0+/t8Zu4Z9+y5uC9vTs7X/e7JOYUsy7IAAAAAACBJR9R7AQAAAAAA6kdJDAAAAACQMCUxAAAAAEDClMQAAAAAAAlTEgMAAAAAJExJDAAAAACQMCUxAAAAAEDCxtV7gQPt378/Xn/99Whubo5CoVDvdQCooSzLYvfu3dHR0RFHHOH3mI1OZgOkS2aPHvIaIF3l5HXDlcSvv/56dHZ21nsNAOqot7c3pkyZUu81OAyZDYDMbnzyGoCR5HXD/cq3ubm53isAUGeyYHTw9wSALGh8/o4AGEkWNFxJ7P/+AoAsGB38PQEgCxqfvyMARpIFVSuJf/jDH0Z3d3ccc8wxMW3atHj66aerdSoAICd5DQCNT14DUG1VKYnvvffeWLJkSVxzzTXx3HPPxac+9amYO3dubN++vRqnAwBykNcA0PjkNQC1UMiyLKv0Hzpjxoz45Cc/GTfffPPQbX/wB38QF154YSxfvvyQswMDA9HS0lLplQAYRfr7+2PChAn1XmPM+yh5HSGzAZDZtSCvAfioRpLXFX8l8d69e2PTpk3R09NTdHtPT0+sXbu25P6Dg4MxMDBQdAAA1VVuXkfIbACoNXkNQK1UvCR+44034v3334/W1tai21tbW6Ovr6/k/suXL4+Wlpaho7Ozs9IrAQAHKDevI2Q2ANSavAagVqr2wXUHfmpelmUH/SS9q6++Ovr7+4eO3t7eaq0EABxgpHkdIbMBoF7kNQDVNq7Sf+CkSZPiyCOPLPmt5s6dO0t++xkR0dTUFE1NTZVeAwA4hHLzOkJmA0CtyWsAaqXiryQ++uijY9q0abFq1aqi21etWhWzZ8+u9OkAgBzkNQA0PnkNQK1U/JXEERFLly6NP//zP4/p06fHrFmz4p/+6Z9i+/bt8dWvfrUapwMAcpDXAND45DUAtVCVkvjiiy+OXbt2xd///d/Hjh074uSTT45HH300urq6qnE6ACAHeQ0AjU9eA1ALhSzLsnov8WEDAwPR0tJS7zUAqKP+/v6YMGFCvdfgMGQ2ADK78clrAEaS1xV/T2IAAAAAAEYPJTEAAAAAQMKUxAAAAAAACVMSAwAAAAAkTEkMAAAAAJAwJTEAAAAAQMKUxAAAAAAACVMSAwAAAAAkTEkMAAAAAJAwJTEAAAAAQMKUxAAAAAAACVMSAwAAAAAkTEkMAAAAAJAwJTEAAAAAQMKUxAAAAAAACVMSAwAAAAAkTEkMAAAAAJAwJTEAAAAAQMKUxAAAAAAACVMSAwAAAAAkTEkMAAAAAJAwJTEAAAAAQMKUxAAAAAAACVMSAwAAAAAkTEkMAAAAAJAwJTEAAAAAQMKUxAAAAAAACRtX7wUAABgdfvSjH5U9M3v27Fznmj59eq65PXv25JoDAICUeSUxAAAAAEDClMQAAAAAAAlTEgMAAAAAJExJDAAAAACQMCUxAAAAAEDClMQAAAAAAAlTEgMAAAAAJExJDAAAAACQMCUxAAAAAEDClMQAAAAAAAlTEgMAAAAAJExJDAAAAACQMCUxAAAAAEDCxtV7AaAyFixYkGvue9/7Xq65zs7OXHN5LF26NNfcjTfeWOFNAMaG6dOn55r7i7/4i7JnNm7cmOtc7733Xq45AIDR6PLLLy975rvf/W6uc82cOTPX3AsvvJBrjtHBK4kBAAAAABKmJAYAAAAASJiSGAAAAAAgYUpiAAAAAICEKYkBAAAAABKmJAYAAAAASJiSGAAAAAAgYUpiAAAAAICEKYkBAAAAABKmJAYAAAAASJiSGAAAAAAgYUpiAAAAAICEKYkBAAAAABI2rt4LAMU6OztzzS1ZsqSyixzG/fffX/bMlClTcp1rxYoVueZmzZqVa27BggW55gBqbcaMGbnmfvCDH+Sa27NnT9kz1157ba5zvf/++7nmAADqqbW1Ndfc8uXLy545+uijc50LDsYriQEAAAAAEqYkBgAAAABIWMVL4mXLlkWhUCg62traKn0aAOAjkNcAMDrIbABqoSrvSXzSSSfFz3/+86GvjzzyyGqcBgD4COQ1AIwOMhuAaqtKSTxu3Di/2QSABievAWB0kNkAVFtV3pN4y5Yt0dHREd3d3fGFL3whtm7dOux9BwcHY2BgoOgAAKqvnLyOkNkAUC9+xgag2ipeEs+YMSPuuOOOeOyxx+LWW2+Nvr6+mD17duzateug91++fHm0tLQMHZ2dnZVeCQA4QLl5HSGzAaAe/IwNQC1UvCSeO3dufP7zn49TTjkl/uRP/iQeeeSRiIi4/fbbD3r/q6++Ovr7+4eO3t7eSq8EAByg3LyOkNkAUA9+xgagFqrynsQfNn78+DjllFNiy5YtB/1+U1NTNDU1VXsNAOAQDpfXETIbABqBn7EBqIaqvCfxhw0ODsZLL70U7e3t1T4VAJCTvAaA0UFmA1ANFS+Jv/71r8eaNWti27Zt8eyzz8b8+fNjYGAgFi5cWOlTAQA5yWsAGB1kNgC1UPG3m/jv//7v+OIXvxhvvPFGHH/88TFz5sxYv359dHV1VfpUAEBO8hoARgeZDUAtVLwkvueeeyr9R8KoleeThH/xi19UYZPhnXHGGbnm8nwARt5PVs57TS666KJcc3n29IEgjDbyemz48pe/nGvutNNOyzX3k5/8pOyZn/70p7nOBcD/kdkwuuT9OfS4444re2b//v25zvX+++/nmmNsq/p7EgMAAAAA0LiUxAAAAAAACVMSAwAAAAAkTEkMAAAAAJAwJTEAAAAAQMKUxAAAAAAACVMSAwAAAAAkTEkMAAAAAJAwJTEAAAAAQMKUxAAAAAAACVMSAwAAAAAkTEkMAAAAAJAwJTEAAAAAQMLG1XsBGMuWLFlS9kxnZ2eucy1dujTXXG9vb665Wp5r6tSpuebuu+++XHPz588ve+bGG2/MdS6Aj2L69Ok1Pd8NN9xQ0/MBANRLR0dHrrm/+7u/q/Amw9u8eXOuuf/8z/+s8CaMBV5JDAAAAACQMCUxAAAAAEDClMQAAAAAAAlTEgMAAAAAJExJDAAAAACQMCUxAAAAAEDClMQAAAAAAAlTEgMAAAAAJExJDAAAAACQMCUxAAAAAEDClMQAAAAAAAlTEgMAAAAAJExJDAAAAACQsHH1XgDGsosuuqhm53rggQdqdq6x7rXXXqvZuRYsWJBrbsaMGbnmrrzyylxzQGPasWNHrrlCoZBr7plnnil7ZuvWrbnOtXnz5lxzHR0dueamT59e9swvfvGLXOd69dVXc839+7//e665xx9/vOyZt956K9e5AGCsmDp1aq65j33sYxXeZHi33HJLzc7F2OeVxAAAAAAACVMSAwAAAAAkTEkMAAAAAJAwJTEAAAAAQMKUxAAAAAAACVMSAwAAAAAkTEkMAAAAAJAwJTEAAAAAQMKUxAAAAAAACVMSAwAAAAAkTEkMAAAAAJAwJTEAAAAAQMKUxAAAAAAACRtX7wVgNOjs7Kz3CuQwZcqUXHPz58+vyUxExMyZM3PN3XjjjbnmgLHl29/+dq65rVu35po744wzyp7JsizXuSZPnpxrbt++fbnm1q9fX/bMpEmTcp1r9uzZueYuueSSXHMDAwNlz9x66625znX99dfnmnvjjTdyzQHA4Rx11FG55r7xjW9UeJPKe+CBB+q9AmOIVxIDAAAAACRMSQwAAAAAkDAlMQAAAABAwpTEAAAAAAAJUxIDAAAAACRMSQwAAAAAkDAlMQAAAABAwpTEAAAAAAAJUxIDAAAAACRMSQwAAAAAkDAlMQAAAABAwpTEAAAAAAAJG1fvBWA0mDVrVq65zs7Osmfuv//+XOfq7e3NNZfXFVdcUZOZiHzXsdbWrVuXa+6MM87INVfrv2+gMb3wwgs1naPY+PHjc821t7fnmvvSl76Ua+6qq64qe+bKK6/Mda4//uM/zjU3d+7cXHM7d+7MNQdAOs4888xcc3/2Z39W4U0O7T/+4z/Knnn77bersAmp8kpiAAAAAICEKYkBAAAAABKmJAYAAAAASFjZJfFTTz0VF1xwQXR0dEShUIiHHnqo6PtZlsWyZcuio6Mjjj322JgzZ068+OKLldoXABgBeQ0AjU9eA9Aoyi6J9+zZE6eeemqsXLnyoN+//vrrY8WKFbFy5crYsGFDtLW1xXnnnRe7d+/+yMsCACMjrwGg8clrABrFuHIH5s6dO+ynD2dZFjfddFNcc801MW/evIiIuP3226O1tTXuuuuu+MpXvvLRtgUARkReA0Djk9cANIqKvifxtm3boq+vL3p6eoZua2pqirPPPjvWrl170JnBwcEYGBgoOgCA6smT1xEyGwBqSV4DUEsVLYn7+voiIqK1tbXo9tbW1qHvHWj58uXR0tIydHR2dlZyJQDgAHnyOkJmA0AtyWsAaqmiJfEHCoVC0ddZlpXc9oGrr746+vv7h47e3t5qrAQAHKCcvI6Q2QBQD/IagFoo+z2JD6WtrS0i/u83nu3t7UO379y5s+S3nx9oamqKpqamSq4BABxCnryOkNkAUEvyGoBaqugribu7u6OtrS1WrVo1dNvevXtjzZo1MXv27EqeCgDISV4DQOOT1wDUUtmvJH777bfjlVdeGfp627Zt8fzzz8fEiRNj6tSpsWTJkrjuuuvihBNOiBNOOCGuu+66OO644+KSSy6p6OIAwPDkNQA0PnkNQKMouyTeuHFjnHPOOUNfL126NCIiFi5cGP/6r/8aV111Vbz77rvxta99Ld58882YMWNGPP7449Hc3Fy5rQGAQ5LXAND45DUAjaKQZVlW7yU+bGBgIFpaWuq9BhRZsGBBrrl777237Jm8Hyxx44035pq74oorcs3V8lOS161bl2vu/vvvzzX3wAMPlD3jA0Eqq7+/PyZMmFDvNTgMmQ2jw0knnVT2zD/8wz/kOtecOXNyzW3cuDHX3OLFi8ueefbZZ3Odi4OT2Y1PXpO6FStW5Jr767/+61xz+/fvzzX3p3/6p2XPfPjtaOBQRpLXFX1PYgAAAAAARhclMQAAAABAwpTEAAAAAAAJUxIDAAAAACRMSQwAAAAAkDAlMQAAAABAwpTEAAAAAAAJUxIDAAAAACRMSQwAAAAAkDAlMQAAAABAwpTEAAAAAAAJUxIDAAAAACRMSQwAAAAAkLBx9V4ARoP58+fX7FydnZ255lasWJFrrre3N9fc/fffX/bMggULcp0LAMgvz39bvPvuu1XYZHjTpk3LNXf++eeXPfPss8/mOhcA9Td+/PiyZ5YsWZLrXFmW5Zp77733cs2tWrUq1xxUilcSAwAAAAAkTEkMAAAAAJAwJTEAAAAAQMKUxAAAAAAACVMSAwAAAAAkTEkMAAAAAJAwJTEAAAAAQMKUxAAAAAAACVMSAwAAAAAkTEkMAAAAAJAwJTEAAAAAQMKUxAAAAAAACVMSAwAAAAAkbFy9F4BauuGGG3LNzZw5s8KbVN6KFStyzd1000255np7e3PNAcBIjB8/PtfcZZddlmtu2rRpueby+NjHPpZr7vzzz6/wJpVXKBRyzb311lu55l555ZVccwCMTvPnz6/3Cod1880313sFyMUriQEAAAAAEqYkBgAAAABImJIYAAAAACBhSmIAAAAAgIQpiQEAAAAAEqYkBgAAAABImJIYAAAAACBhSmIAAAAAgIQpiQEAAAAAEqYkBgAAAABImJIYAAAAACBhSmIAAAAAgIQVsizL6r3Ehw0MDERLS0u916DBrV27NtfcrFmzKrxJ4ygUCvVeASqmv78/JkyYUO81OAyZTTV973vfyzV3xRVXVHiT4eXN3rz/+f3CCy/kmnvvvffKnvnEJz6R61y/+7u/m2tu/fr1ueZmz56da47KkdmNT17TiCZOnJhrbvPmzWXPdHR05DrXjh07cs2deuqpueZ+85vf5JqDkRhJXnslMQAAAABAwpTEAAAAAAAJUxIDAAAAACRMSQwAAAAAkDAlMQAAAABAwpTEAAAAAAAJUxIDAAAAACRMSQwAAAAAkDAlMQAAAABAwpTEAAAAAAAJUxIDAAAAACRMSQwAAAAAkDAlMQAAAABAwsbVewG47777yp6ZNWtWFTYZ3ooVK3LNXXTRRWXPdHZ25jpX3rne3t5ccwBQTZs2bco1t2XLllxzTz75ZNkzDz74YK5zbdu2Ldfcr3/961xz//u//1v2zCc+8Ylc53rppZdyzQEwOh177LG55q644opcc21tbWXPZFmW61yrV6/ONfeb3/wm1xzUm1cSAwAAAAAkTEkMAAAAAJAwJTEAAAAAQMLKLomfeuqpuOCCC6KjoyMKhUI89NBDRd+/9NJLo1AoFB0zZ86s1L4AwAjIawBofPIagEZRdkm8Z8+eOPXUU2PlypXD3uf888+PHTt2DB2PPvroR1oSACiPvAaAxievAWgU48odmDt3bsydO/eQ92lqahrxJ04ODg7G4ODg0NcDAwPlrgQAHKDSeR0hswGg0uQ1AI2iKu9JvHr16pg8eXKceOKJcdlll8XOnTuHve/y5cujpaVl6Ojs7KzGSgDAAcrJ6wiZDQD1IK8BqIWKl8Rz586NO++8M5544om44YYbYsOGDXHuuecW/Sbzw66++uro7+8fOnp7eyu9EgBwgHLzOkJmA0CtyWsAaqXst5s4nIsvvnjon08++eSYPn16dHV1xSOPPBLz5s0ruX9TU1M0NTVVeg0A4BDKzesImQ0AtSavAaiVqrzdxIe1t7dHV1dXbNmypdqnAgByktcA0PjkNQDVUvWSeNeuXdHb2xvt7e3VPhUAkJO8BoDGJ68BqJay327i7bffjldeeWXo623btsXzzz8fEydOjIkTJ8ayZcvi85//fLS3t8err74a3/rWt2LSpEnxuc99rqKLAwDDk9cA0PjkNQCNouySeOPGjXHOOecMfb106dKIiFi4cGHcfPPNsXnz5rjjjjvirbfeivb29jjnnHPi3nvvjebm5sptDQAckrwGgMYnrwFoFGWXxHPmzIksy4b9/mOPPfaRFmL0uuKKK3LNXXTRRWXP5P2E3gULFuSaW79+fa65zs7OmsxERMyfPz/X3I033phrDmhs8prR7u67767pHMWmT59e7xUgCfKa0e6b3/xmrrlvfetbFd6k8p5//vl6rwA1VfX3JAYAAAAAoHEpiQEAAAAAEqYkBgAAAABImJIYAAAAACBhSmIAAAAAgIQpiQEAAAAAEqYkBgAAAABImJIYAAAAACBhSmIAAAAAgIQpiQEAAAAAEqYkBgAAAABImJIYAAAAACBhSmIAAAAAgISNq/cCNJ6ZM2fmmluxYkWFNxneggULcs2tX7++wps0jtdee63eKwAAAJCQ0047rd4rHNZPf/rTXHMrV66s8CbQ2LySGAAAAAAgYUpiAAAAAICEKYkBAAAAABKmJAYAAAAASJiSGAAAAAAgYUpiAAAAAICEKYkBAAAAABKmJAYAAAAASJiSGAAAAAAgYUpiAAAAAICEKYkBAAAAABKmJAYAAAAASNi4ei9A45k1a1ZNz7du3bqyZ1577bUqbFJ5vb29NTvXxz/+8ZqdCwAY2770pS/lmisUCrnm7r777lxzAFTGOeeck2vu05/+dIU3ObT/+Z//KXvmb/7mb3Kd65133sk1B6OVVxIDAAAAACRMSQwAAAAAkDAlMQAAAABAwpTEAAAAAAAJUxIDAAAAACRMSQwAAAAAkDAlMQAAAABAwpTEAAAAAAAJUxIDAAAAACRMSQwAAAAAkDAlMQAAAABAwpTEAAAAAAAJUxIDAAAAACRsXL0XoPFMmTKlpue7//77y57p7e2twiYAAI3rmGOOyTV3zTXXlD1z7rnn5jpXlmW55jZs2JBrDoBSRxxR/usBv/GNb+Q617hxta2Vvvvd75Y986tf/aoKm8DY45XEAAAAAAAJUxIDAAAAACRMSQwAAAAAkDAlMQAAAABAwpTEAAAAAAAJUxIDAAAAACRMSQwAAAAAkDAlMQAAAABAwpTEAAAAAAAJUxIDAAAAACRMSQwAAAAAkDAlMQAAAABAwpTEAAAAAAAJG1fvBQAAUnf00Ufnmps6dWqFNzm0nTt3lj2zd+/eXOcaP358rrn3338/19ykSZPKnjnjjDNynau7uzvX3Lx583LNnXTSSWXP7N69O9e5Fi9enGtuw4YNueYAKHXaaaeVPXPeeedVYZPhbdy4Mdfcv/zLv1R4E+ADXkkMAAAAAJAwJTEAAAAAQMLKKomXL18ep59+ejQ3N8fkyZPjwgsvjJdffrnoPlmWxbJly6KjoyOOPfbYmDNnTrz44osVXRoAGJ68BoDRQWYD0CjKKonXrFkTixYtivXr18eqVati37590dPTE3v27Bm6z/XXXx8rVqyIlStXxoYNG6KtrS3OO++83O9rBgCUR14DwOggswFoFGV9cN3Pfvazoq9vu+22mDx5cmzatCnOOuusyLIsbrrpprjmmmuGPljj9ttvj9bW1rjrrrviK1/5SuU2BwAOSl4DwOggswFoFB/pPYn7+/sjImLixIkREbFt27bo6+uLnp6eofs0NTXF2WefHWvXrj3onzE4OBgDAwNFBwBQOZXI6wiZDQDV5mdsAOold0mcZVksXbo0zjzzzDj55JMjIqKvry8iIlpbW4vu29raOvS9Ay1fvjxaWlqGjs7OzrwrAQAHqFReR8hsAKgmP2MDUE+5S+LLL788fvnLX8bdd99d8r1CoVD0dZZlJbd94Oqrr47+/v6ho7e3N+9KAMABKpXXETIbAKrJz9gA1FNZ70n8gcWLF8fDDz8cTz31VEyZMmXo9ra2toj4v992tre3D92+c+fOkt98fqCpqSmampryrAEAHEIl8zpCZgNAtfgZG4B6K+uVxFmWxeWXXx4PPvhgPPHEE9Hd3V30/e7u7mhra4tVq1YN3bZ3795Ys2ZNzJ49uzIbAwCHJK8BYHSQ2QA0irJeSbxo0aK466674ic/+Uk0NzcPvQdSS0tLHHvssVEoFGLJkiVx3XXXxQknnBAnnHBCXHfddXHcccfFJZdcUpV/AQCgmLwGgNFBZgPQKMoqiW+++eaIiJgzZ07R7bfddltceumlERFx1VVXxbvvvhtf+9rX4s0334wZM2bE448/Hs3NzRVZGAA4NHkNAKODzAagUZRVEmdZdtj7FAqFWLZsWSxbtizvTgDARyCvAWB0kNkANIpcH1wHlXTRRReVPfPAAw/kOletP9l31qxZNTvXa6+9VrNzAVBZf/mXf5lr7vvf/36FNzm0rVu3lj3z3nvv5TpXS0tLrrl9+/blmuvq6ip7plAo5DrXSEqhg3nrrbdyzd11111lz/zjP/5jrnM988wzueYAqJxbbrml3isc1p133plr7o033qjwJsAHyvrgOgAAAAAAxhYlMQAAAABAwpTEAAAAAAAJUxIDAAAAACRMSQwAAAAAkDAlMQAAAABAwpTEAAAAAAAJUxIDAAAAACRMSQwAAAAAkDAlMQAAAABAwpTEAAAAAAAJUxIDAAAAACSskGVZVu8lPmxgYCBaWlrqvQY5bN++PddcZ2dnhTcZ3rp163LNTZkyJddcnn+33t7eXOeaOnVqrjloRP39/TFhwoR6r8FhyOzK+cM//MNcc0uXLq3sIofx5S9/ueyZBvtPzWE9+eSTZc/s2LEj17k2btyYa+6f//mfc83t2bMn1xyMhMxufPJ69PrMZz6Ta+6+++4re6apqSnXuf7rv/4r19yZZ56Za27Xrl255iB1I8lrryQGAAAAAEiYkhgAAAAAIGFKYgAAAACAhCmJAQAAAAASpiQGAAAAAEiYkhgAAAAAIGFKYgAAAACAhCmJAQAAAAASpiQGAAAAAEiYkhgAAAAAIGFKYgAAAACAhCmJAQAAAAASpiQGAAAAAEhYIcuyrN5LfNjAwEC0tLTUew1q6L777it7ZubMmbnO1dnZmWsur3Xr1pU9s3Tp0lznWr9+fa45aET9/f0xYcKEeq/BYchsAGR245PXo9fUqVNzzW3atKnsmYkTJ+Y616c//elccz//+c9zzQH5jCSvvZIYAAAAACBhSmIAAAAAgIQpiQEAAAAAEqYkBgAAAABImJIYAAAAACBhSmIAAAAAgIQpiQEAAAAAEqYkBgAAAABImJIYAAAAACBhSmIAAAAAgIQpiQEAAAAAEqYkBgAAAABImJIYAAAAACBh4+q9ACxYsKDeKwAAAEBD2b59e665448/vsKbACnwSmIAAAAAgIQpiQEAAAAAEqYkBgAAAABImJIYAAAAACBhSmIAAAAAgIQpiQEAAAAAEqYkBgAAAABImJIYAAAAACBhSmIAAAAAgIQpiQEAAAAAEqYkBgAAAABImJIYAAAAACBhSmIAAAAAgIQpiQEAAAAAEqYkBgAAAABImJIYAAAAACBhZZXEy5cvj9NPPz2am5tj8uTJceGFF8bLL79cdJ9LL700CoVC0TFz5syKLg0ADE9eA8DoILMBaBRllcRr1qyJRYsWxfr162PVqlWxb9++6OnpiT179hTd7/zzz48dO3YMHY8++mhFlwYAhievAWB0kNkANIpx5dz5Zz/7WdHXt912W0yePDk2bdoUZ5111tDtTU1N0dbWVpkNAYCyyGsAGB1kNgCN4iO9J3F/f39EREycOLHo9tWrV8fkyZPjxBNPjMsuuyx27tw57J8xODgYAwMDRQcAUDmVyOsImQ0A1eZnbADqpZBlWZZnMMuy+OxnPxtvvvlmPP3000O333vvvfE7v/M70dXVFdu2bYu//du/jX379sWmTZuiqamp5M9ZtmxZfPvb387/bwDAmNPf3x8TJkyo9xpjQqXyOkJmA1BKZleOn7EBqJaR5HXuknjRokXxyCOPxDPPPBNTpkwZ9n47duyIrq6uuOeee2LevHkl3x8cHIzBwcGhrwcGBqKzszPPSgCMEX7grJxK5XWEzAaglMyuHD9jA1AtI8nrst6T+AOLFy+Ohx9+OJ566qlDhldERHt7e3R1dcWWLVsO+v2mpqZhX7EEAORXybyOkNkAUC1+xgag3soqibMsi8WLF8ePf/zjWL16dXR3dx92ZteuXdHb2xvt7e25lwQARk5eA8DoILMBaBRlfXDdokWL4t/+7d/irrvuiubm5ujr64u+vr549913IyLi7bffjq9//euxbt26ePXVV2P16tVxwQUXxKRJk+Jzn/tcVf4FAIBi8hoARgeZDUDDyMoQEQc9brvttizLsuydd97Jenp6suOPPz476qijsqlTp2YLFy7Mtm/fPuJz9Pf3D3seh8PhcKRx9Pf3lxNPHGC461rJvM4yme1wOBwOmf1RDXdd/YztcDgcjkoeI8nr3B9cVy0DAwPR0tJS7zUAqCMfgjM6yGwAZHbjk9cAjCSvy3q7CQAAAAAAxhYlMQAAAABAwpTEAAAAAAAJUxIDAAAAACRMSQwAAAAAkDAlMQAAAABAwpTEAAAAAAAJUxIDAAAAACRMSQwAAAAAkDAlMQAAAABAwpTEAAAAAAAJUxIDAAAAACRMSQwAAAAAkDAlMQAAAABAwpTEAAAAAAAJUxIDAAAAACRMSQwAAAAAkDAlMQAAAABAwpTEAAAAAAAJUxIDAAAAACRMSQwAAAAAkDAlMQAAAABAwpTEAAAAAAAJUxIDAAAAACRMSQwAAAAAkDAlMQAAAABAwhquJM6yrN4rAFBnsmB08PcEgCxofP6OABhJFjRcSbx79+56rwBAncmC0cHfEwCyoPH5OwJgJFlQyBrs14r79++P119/PZqbm6NQKBR9b2BgIDo7O6O3tzcmTJhQpw0bi2tSyjUp5ZoUcz1KNco1ybIsdu/eHR0dHXHEEQ33e0wOILNHzvUo5ZqUck1KuSbFGul6yOzRQ16XxzUp5ZoUcz1KuSalGuWalJPX42q004gdccQRMWXKlEPeZ8KECR50B3BNSrkmpVyTYq5HqUa4Ji0tLXU9PyMns8vnepRyTUq5JqVck2KNcj1k9uggr/NxTUq5JsVcj1KuSalGuCYjzWu/8gUAAAAASJiSGAAAAAAgYaOqJG5qaoprr702mpqa6r1Kw3BNSrkmpVyTYq5HKdeESvOYKuZ6lHJNSrkmpVyTYq4HleYxVco1KeWaFHM9SrkmpUbjNWm4D64DAAAAAKB2RtUriQEAAAAAqCwlMQAAAABAwpTEAAAAAAAJUxIDAAAAACRMSQwAAAAAkLBRVRL/8Ic/jO7u7jjmmGNi2rRp8fTTT9d7pbpZtmxZFAqFoqOtra3ea9XUU089FRdccEF0dHREoVCIhx56qOj7WZbFsmXLoqOjI4499tiYM2dOvPjii/VZtgYOdz0uvfTSksfMzJkz67NsDSxfvjxOP/30aG5ujsmTJ8eFF14YL7/8ctF9UnuMjOSapPY4oTrk9W/Ja3l9MDK7mMwuJbOpBXn9W/JaXh+MvC4mr0uNtbweNSXxvffeG0uWLIlrrrkmnnvuufjUpz4Vc+fOje3bt9d7tbo56aSTYseOHUPH5s2b671STe3ZsydOPfXUWLly5UG/f/3118eKFSti5cqVsWHDhmhra4vzzjsvdu/eXeNNa+Nw1yMi4vzzzy96zDz66KM13LC21qxZE4sWLYr169fHqlWrYt++fdHT0xN79uwZuk9qj5GRXJOItB4nVJ68LiWv5fWBZHYxmV1KZlNt8rqUvJbXB5LXxeR1qTGX19ko8Ud/9EfZV7/61aLbfv/3fz/75je/WaeN6uvaa6/NTj311Hqv0TAiIvvxj3889PX+/fuztra27Dvf+c7Qbe+9917W0tKS3XLLLXXYsLYOvB5ZlmULFy7MPvvZz9Zln0awc+fOLCKyNWvWZFnmMZJlpdckyzxO+OjkdTF5XUxel5LZpWR2KZlNpcnrYvK6mLwuJa9LyetSoz2vR8Uriffu3RubNm2Knp6eott7enpi7dq1ddqq/rZs2RIdHR3R3d0dX/jCF2Lr1q31XqlhbNu2Lfr6+ooeM01NTXH22Wcn/ZhZvXp1TJ48OU488cS47LLLYufOnfVeqWb6+/sjImLixIkR4TESUXpNPpDy44SPRl4fnLwenufi4aX8XCyzS8lsKkleH5y8Hp7n4eGl/Dwsr0uN9rweFSXxG2+8Ee+//360trYW3d7a2hp9fX112qq+ZsyYEXfccUc89thjceutt0ZfX1/Mnj07du3aVe/VGsIHjwuPmd+aO3du3HnnnfHEE0/EDTfcEBs2bIhzzz03BgcH671a1WVZFkuXLo0zzzwzTj755IjwGDnYNYlI+3HCRyevS8nrQ0v9uXg4KT8Xy+xSMptKk9el5PWhpf48PJyUn4fldamxkNfj6r1AOQqFQtHXWZaV3JaKuXPnDv3zKaecErNmzYrf+73fi9tvvz2WLl1ax80ai8fMb1188cVD/3zyySfH9OnTo6urKx555JGYN29eHTervssvvzx++ctfxjPPPFPyvVQfI8Ndk5QfJ1ROqv+7Ohh5PTIeM8VSfi6W2aVkNtWS6v+mDkZej4zHTLGUn4fldamxkNej4pXEkyZNiiOPPLLkNw87d+4s+Q1FqsaPHx+nnHJKbNmypd6rNIQPPonWY2Z47e3t0dXVNeYfM4sXL46HH344nnzyyZgyZcrQ7Sk/Roa7JgeTyuOEypDXhyevi6X8XFyOVJ6LZXYpmU01yOvDk9fFUn4eLkcqz8PyutRYyetRURIfffTRMW3atFi1alXR7atWrYrZs2fXaavGMjg4GC+99FK0t7fXe5WG0N3dHW1tbUWPmb1798aaNWs8Zv6/Xbt2RW9v75h9zGRZFpdffnk8+OCD8cQTT0R3d3fR91N8jBzumhzMWH+cUFny+vDkdbEUn4vzGOvPxTK7lMymmuT14cnrYik+D+cx1p+H5XWpMZfXNf2YvI/gnnvuyY466qjsRz/6UfarX/0qW7JkSTZ+/Pjs1VdfrfdqdXHllVdmq1evzrZu3ZqtX78++8xnPpM1NzcndT12796dPffcc9lzzz2XRUS2YsWK7Lnnnst+/etfZ1mWZd/5zneylpaW7MEHH8w2b96cffGLX8za29uzgYGBOm9eHYe6Hrt3786uvPLKbO3atdm2bduyJ598Mps1a1b28Y9/fMxej7/6q7/KWlpastWrV2c7duwYOt55552h+6T2GDncNUnxcULlyeti8lpeH4zMLiazS8lsqk1eF5PX8vpg5HUxeV1qrOX1qCmJsyzLfvCDH2RdXV3Z0UcfnX3yk5/M1qxZU++V6ubiiy/O2tvbs6OOOirr6OjI5s2bl7344ov1XqumnnzyySwiSo6FCxdmWZZl+/fvz6699tqsra0ta2pqys4666xs8+bN9V26ig51Pd55552sp6cnO/7447Ojjjoqmzp1arZw4cJs+/bt9V67ag52LSIiu+2224buk9pj5HDXJMXHCdUhr39LXsvrg5HZxWR2KZlNLcjr35LX8vpg5HUxeV1qrOV1IcuyLP/rkAEAAAAAGM1GxXsSAwAAAABQHUpiAAAAAICEKYkBAAAAABKmJAYAAAAASJiSGAAAAAAgYUpiAAAAAICEKYkBAAAAABKmJAYAAAAASJiSGAAAAAAgYUpiAAAAAICEKYkBAAAAABL2/wBMNored1oANAAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 1800x500 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(18, 5))\n",
    "for i in range(3):\n",
    "    plt.subplot(1, 3, i + 1)\n",
    "    plt.imshow(X_train.iloc[i].values.reshape(28,28), cmap='gray')\n",
    "    plt.subplots_adjust(wspace=0.2, hspace=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_size = 10\n",
    "input_size = X_train.columns[0:].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Linear(input_size,64), # input\n",
    "    nn.ReLU(), # activation\n",
    "    nn.Linear(64,32), # hidden layer\n",
    "    nn.ReLU(), # activation\n",
    "    nn.Linear(32,output_size), # ouput\n",
    ")\n",
    "\n",
    "criterion = nn.CrossEntropyLoss() # loss function\n",
    "optimizer = optim.Adam(model.parameters(),lr=0.001) # optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique labels in y_train: tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n"
     ]
    }
   ],
   "source": [
    "print(\"Unique labels in y_train:\", torch.unique(y_train_tensors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Number of epochs\n",
    "# num_epochs = 100  \n",
    "\n",
    "# for epoch in range(num_epochs):\n",
    "#     # training mode\n",
    "#     model.train()\n",
    "\n",
    "#     # Loop over batches\n",
    "#     for X_batch, y_batch in train_loader:\n",
    "#         # Forward pass: compute the output of the model for the batch\n",
    "#         outputs = model(X_batch)\n",
    "\n",
    "#         # Compute the loss for the batch\n",
    "#         loss = criterion(outputs, y_batch)\n",
    "\n",
    "#         # Backward pass: compute gradients\n",
    "#         optimizer.zero_grad()  # Clear previous gradients\n",
    "#         loss.backward()        # Backpropagation\n",
    "\n",
    "#         # Update the weights\n",
    "#         optimizer.step()\n",
    "\n",
    "#     # Print the loss every 10 epochs\n",
    "#     if (epoch + 1) % 10 == 0:\n",
    "#         print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}, Accuracy: {accuracy_score(y_batch)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_nn(model,loader):\n",
    "    num_epochs = 100\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  # Set model to training mode\n",
    "\n",
    "        # Variables to accumulate predictions and true labels\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            # Forward pass: compute the output of the model\n",
    "            outputs = model(X_batch)\n",
    "            \n",
    "            # Compute the loss\n",
    "            loss = criterion(outputs, y_batch)\n",
    "\n",
    "            # Backward pass: compute gradients\n",
    "            optimizer.zero_grad()  # Clear previous gradients\n",
    "            loss.backward()        # Backpropagation\n",
    "\n",
    "            # Update the weights\n",
    "            optimizer.step()\n",
    "\n",
    "            # Convert model outputs to predicted class indices\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "            # Accumulate predictions and true labels\n",
    "            all_preds.extend(predicted.cpu().numpy())  # Convert to numpy and store\n",
    "            all_labels.extend(y_batch.cpu().numpy())   # Convert to numpy and store\n",
    "\n",
    "        # Compute metrics at the end of the epoch\n",
    "        accuracy = accuracy_score(all_labels, all_preds)\n",
    "        f1 = f1_score(all_labels, all_preds, average='weighted')  # For multi-class problems\n",
    "\n",
    "        # Print loss, accuracy, and F1 score every 10 epochs\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}, Accuracy: {accuracy:.4f}, F1 Score: {f1:.4f}')\n",
    "        \n",
    "    return loss.item(), accuracy, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Loss: 0.0000, Accuracy: 0.9993, F1 Score: 0.9993\n",
      "Epoch [20/100], Loss: 0.0001, Accuracy: 0.9966, F1 Score: 0.9966\n",
      "Epoch [30/100], Loss: 0.0000, Accuracy: 0.9974, F1 Score: 0.9974\n",
      "Epoch [40/100], Loss: 0.0002, Accuracy: 0.9983, F1 Score: 0.9983\n",
      "Epoch [50/100], Loss: 0.0002, Accuracy: 0.9982, F1 Score: 0.9982\n",
      "Epoch [60/100], Loss: 0.0000, Accuracy: 0.9980, F1 Score: 0.9980\n",
      "Epoch [70/100], Loss: 0.0000, Accuracy: 0.9982, F1 Score: 0.9982\n",
      "Epoch [80/100], Loss: 0.1468, Accuracy: 0.9978, F1 Score: 0.9978\n",
      "Epoch [90/100], Loss: 0.0000, Accuracy: 0.9980, F1 Score: 0.9980\n",
      "Epoch [100/100], Loss: 0.0001, Accuracy: 0.9968, F1 Score: 0.9969\n",
      "0.0001, 0.9968488762728196, 0.9968561840675131\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy, f1 = train_nn(model,train_loader)\n",
    "\n",
    "print(f'{loss:.4f}, {accuracy}, {f1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Assuming num_classes is defined and your model is set up correctly\n",
    "# num_epochs = 100\n",
    "\n",
    "# for epoch in range(num_epochs):\n",
    "#     model.train()  # Set model to training mode\n",
    "\n",
    "#     # Variables to accumulate predictions and true labels\n",
    "#     all_preds = []\n",
    "#     all_labels = []\n",
    "\n",
    "#     for X_batch, y_batch in train_loader:\n",
    "#         # Forward pass: compute the output of the model\n",
    "#         outputs = model(X_batch)\n",
    "        \n",
    "#         # Compute the loss\n",
    "#         loss = criterion(outputs, y_batch)\n",
    "\n",
    "#         # Backward pass: compute gradients\n",
    "#         optimizer.zero_grad()  # Clear previous gradients\n",
    "#         loss.backward()        # Backpropagation\n",
    "\n",
    "#         # Update the weights\n",
    "#         optimizer.step()\n",
    "\n",
    "#         # Convert model outputs to predicted class indices\n",
    "#         _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "#         # Accumulate predictions and true labels\n",
    "#         all_preds.extend(predicted.cpu().numpy())  # Convert to numpy and store\n",
    "#         all_labels.extend(y_batch.cpu().numpy())   # Convert to numpy and store\n",
    "\n",
    "#     # Compute metrics at the end of the epoch\n",
    "#     accuracy = accuracy_score(all_labels, all_preds)\n",
    "#     f1 = f1_score(all_labels, all_preds, average='weighted')  # For multi-class problems\n",
    "\n",
    "#     # Print loss, accuracy, and F1 score every 10 epochs\n",
    "#     if (epoch + 1) % 10 == 0:\n",
    "#         print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}, Accuracy: {accuracy:.4f}, F1 Score: {f1:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation_nn(model, X_set, y_set):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():  # No need to calculate gradients for validation/testing\n",
    "        outputs = model(X_set)\n",
    "        val_loss = criterion(outputs, y_set)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        all_preds.extend(predicted.cpu().numpy())  # Convert to numpy and store\n",
    "        all_labels.extend(y_set.cpu().numpy())   # Convert to numpy and store\n",
    "        accuracy = accuracy_score(all_labels, all_preds)\n",
    "        f1 = f1_score(all_labels, all_preds, average='weighted')  # For multi-class problems\n",
    "        \n",
    "        print(f'Validation Loss: {val_loss.item():.4f}, Accuracy: {accuracy:.4f}, F1 Score: {f1:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.eval()  # Set the model to evaluation mode\n",
    "# with torch.no_grad():  # No need to calculate gradients for validation/testing\n",
    "#     outputs = model(X_validation_tensors)\n",
    "#     val_loss = criterion(outputs, y_validation_tensors)\n",
    "#     print(f'Validation Loss: {val_loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 3.5958, Accuracy: 0.9620, F1 Score: 0.9620\n"
     ]
    }
   ],
   "source": [
    "evaluation_nn(model,X_validation_tensors, y_validation_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 3.2595, Accuracy: 0.9662, F1 Score: 0.9662\n"
     ]
    }
   ],
   "source": [
    "evaluation_nn(model,X_test_tensors, y_test_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.eval()  # Set the model to evaluation mode\n",
    "# all_preds = []\n",
    "# all_labels = []\n",
    "\n",
    "# with torch.no_grad():  # No need to calculate gradients for validation/testing\n",
    "#     outputs = model(X_test_tensors)\n",
    "#     val_loss = criterion(outputs, y_test_tensors)\n",
    "#     print(f'Test Loss: {val_loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2 = nn.Sequential(\n",
    "    nn.Linear(input_size,64), # input\n",
    "    nn.ReLU(), # activation\n",
    "    nn.Linear(64,32), # hidden layer\n",
    "    nn.ReLU(), # activation\n",
    "    nn.Linear(32,64),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(32,output_size), # ouput\n",
    ")\n",
    "\n",
    "criterion = nn.CrossEntropyLoss() # loss function\n",
    "optimizer = optim.Adam(model.parameters(),lr=0.001) # optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (64x10 and 32x10)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/amandanitta/Desktop/GitHub/ICS661/ics661_assignment1/mlp.ipynb Cell 23\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/amandanitta/Desktop/GitHub/ICS661/ics661_assignment1/mlp.ipynb#X41sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m train_nn(model_2,train_loader)\n",
      "\u001b[1;32m/Users/amandanitta/Desktop/GitHub/ICS661/ics661_assignment1/mlp.ipynb Cell 23\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/amandanitta/Desktop/GitHub/ICS661/ics661_assignment1/mlp.ipynb#X41sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m all_labels \u001b[39m=\u001b[39m []\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/amandanitta/Desktop/GitHub/ICS661/ics661_assignment1/mlp.ipynb#X41sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39mfor\u001b[39;00m X_batch, y_batch \u001b[39min\u001b[39;00m train_loader:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/amandanitta/Desktop/GitHub/ICS661/ics661_assignment1/mlp.ipynb#X41sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     \u001b[39m# Forward pass: compute the output of the model\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/amandanitta/Desktop/GitHub/ICS661/ics661_assignment1/mlp.ipynb#X41sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     outputs \u001b[39m=\u001b[39m model(X_batch)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/amandanitta/Desktop/GitHub/ICS661/ics661_assignment1/mlp.ipynb#X41sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     \u001b[39m# Compute the loss\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/amandanitta/Desktop/GitHub/ICS661/ics661_assignment1/mlp.ipynb#X41sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     loss \u001b[39m=\u001b[39m criterion(outputs, y_batch)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1564\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.12/site-packages/torch/nn/modules/container.py:219\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    218\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 219\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39m)\n\u001b[1;32m    220\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1564\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.12/site-packages/torch/nn/modules/linear.py:117\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 117\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mlinear(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweight, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (64x10 and 32x10)"
     ]
    }
   ],
   "source": [
    "train_nn(model_2,train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
